{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "\n",
    "The `Hello World` program for machine learning.\n",
    "\n",
    "Which is used to read handwriting numbers in images.\n",
    "\n",
    "For example, the following 4 images have handwriting number in each image:\n",
    "![MNIST](assets/MNIST.png)\n",
    "\n",
    "And we are going to write a model to recognize them, by using `softmax regression`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data sets\n",
    "\n",
    "Run the code below to import the functions for data set management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2015 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Functions for downloading and reading MNIST data.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import gzip\n",
    "import os\n",
    "import tensorflow.python.platform\n",
    "import numpy\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\n",
    "def maybe_download(filename, work_directory):\n",
    "  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n",
    "  if not os.path.exists(work_directory):\n",
    "    os.mkdir(work_directory)\n",
    "  filepath = os.path.join(work_directory, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  return filepath\n",
    "def _read32(bytestream):\n",
    "  dt = numpy.dtype(numpy.uint32).newbyteorder('>')\n",
    "  return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n",
    "def extract_images(filename):\n",
    "  \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    magic = _read32(bytestream)\n",
    "    if magic != 2051:\n",
    "      raise ValueError(\n",
    "          'Invalid magic number %d in MNIST image file: %s' %\n",
    "          (magic, filename))\n",
    "    num_images = _read32(bytestream)\n",
    "    rows = _read32(bytestream)\n",
    "    cols = _read32(bytestream)\n",
    "    buf = bytestream.read(rows * cols * num_images)\n",
    "    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "    data = data.reshape(num_images, rows, cols, 1)\n",
    "    return data\n",
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "  \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "  num_labels = labels_dense.shape[0]\n",
    "  index_offset = numpy.arange(num_labels) * num_classes\n",
    "  labels_one_hot = numpy.zeros((num_labels, num_classes))\n",
    "  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "  return labels_one_hot\n",
    "def extract_labels(filename, one_hot=False):\n",
    "  \"\"\"Extract the labels into a 1D uint8 numpy array [index].\"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    magic = _read32(bytestream)\n",
    "    if magic != 2049:\n",
    "      raise ValueError(\n",
    "          'Invalid magic number %d in MNIST label file: %s' %\n",
    "          (magic, filename))\n",
    "    num_items = _read32(bytestream)\n",
    "    buf = bytestream.read(num_items)\n",
    "    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "    if one_hot:\n",
    "      return dense_to_one_hot(labels)\n",
    "    return labels\n",
    "class DataSet(object):\n",
    "  def __init__(self, images, labels, fake_data=False, one_hot=False,\n",
    "               dtype=tf.float32):\n",
    "    \"\"\"Construct a DataSet.\n",
    "    one_hot arg is used only if fake_data is true.  `dtype` can be either\n",
    "    `uint8` to leave the input as `[0, 255]`, or `float32` to rescale into\n",
    "    `[0, 1]`.\n",
    "    \"\"\"\n",
    "    dtype = tf.as_dtype(dtype).base_dtype\n",
    "    if dtype not in (tf.uint8, tf.float32):\n",
    "      raise TypeError('Invalid image dtype %r, expected uint8 or float32' %\n",
    "                      dtype)\n",
    "    if fake_data:\n",
    "      self._num_examples = 10000\n",
    "      self.one_hot = one_hot\n",
    "    else:\n",
    "      assert images.shape[0] == labels.shape[0], (\n",
    "          'images.shape: %s labels.shape: %s' % (images.shape,\n",
    "                                                 labels.shape))\n",
    "      self._num_examples = images.shape[0]\n",
    "      # Convert shape from [num examples, rows, columns, depth]\n",
    "      # to [num examples, rows*columns] (assuming depth == 1)\n",
    "      assert images.shape[3] == 1\n",
    "      images = images.reshape(images.shape[0],\n",
    "                              images.shape[1] * images.shape[2])\n",
    "      if dtype == tf.float32:\n",
    "        # Convert from [0, 255] -> [0.0, 1.0].\n",
    "        images = images.astype(numpy.float32)\n",
    "        images = numpy.multiply(images, 1.0 / 255.0)\n",
    "    self._images = images\n",
    "    self._labels = labels\n",
    "    self._epochs_completed = 0\n",
    "    self._index_in_epoch = 0\n",
    "  @property\n",
    "  def images(self):\n",
    "    return self._images\n",
    "  @property\n",
    "  def labels(self):\n",
    "    return self._labels\n",
    "  @property\n",
    "  def num_examples(self):\n",
    "    return self._num_examples\n",
    "  @property\n",
    "  def epochs_completed(self):\n",
    "    return self._epochs_completed\n",
    "  def next_batch(self, batch_size, fake_data=False):\n",
    "    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "    if fake_data:\n",
    "      fake_image = [1] * 784\n",
    "      if self.one_hot:\n",
    "        fake_label = [1] + [0] * 9\n",
    "      else:\n",
    "        fake_label = 0\n",
    "      return [fake_image for _ in xrange(batch_size)], [\n",
    "          fake_label for _ in xrange(batch_size)]\n",
    "    start = self._index_in_epoch\n",
    "    self._index_in_epoch += batch_size\n",
    "    if self._index_in_epoch > self._num_examples:\n",
    "      # Finished epoch\n",
    "      self._epochs_completed += 1\n",
    "      # Shuffle the data\n",
    "      perm = numpy.arange(self._num_examples)\n",
    "      numpy.random.shuffle(perm)\n",
    "      self._images = self._images[perm]\n",
    "      self._labels = self._labels[perm]\n",
    "      # Start next epoch\n",
    "      start = 0\n",
    "      self._index_in_epoch = batch_size\n",
    "      assert batch_size <= self._num_examples\n",
    "    end = self._index_in_epoch\n",
    "    return self._images[start:end], self._labels[start:end]\n",
    "def read_data_sets(train_dir, fake_data=False, one_hot=False, dtype=tf.float32):\n",
    "  class DataSets(object):\n",
    "    pass\n",
    "  data_sets = DataSets()\n",
    "  if fake_data:\n",
    "    def fake():\n",
    "      return DataSet([], [], fake_data=True, one_hot=one_hot, dtype=dtype)\n",
    "    data_sets.train = fake()\n",
    "    data_sets.validation = fake()\n",
    "    data_sets.test = fake()\n",
    "    return data_sets\n",
    "  TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n",
    "  TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n",
    "  TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
    "  TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
    "  VALIDATION_SIZE = 5000\n",
    "  local_file = maybe_download(TRAIN_IMAGES, train_dir)\n",
    "  train_images = extract_images(local_file)\n",
    "  local_file = maybe_download(TRAIN_LABELS, train_dir)\n",
    "  train_labels = extract_labels(local_file, one_hot=one_hot)\n",
    "  local_file = maybe_download(TEST_IMAGES, train_dir)\n",
    "  test_images = extract_images(local_file)\n",
    "  local_file = maybe_download(TEST_LABELS, train_dir)\n",
    "  test_labels = extract_labels(local_file, one_hot=one_hot)\n",
    "  validation_images = train_images[:VALIDATION_SIZE]\n",
    "  validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "  train_images = train_images[VALIDATION_SIZE:]\n",
    "  train_labels = train_labels[VALIDATION_SIZE:]\n",
    "  data_sets.train = DataSet(train_images, train_labels, dtype=dtype)\n",
    "  data_sets.validation = DataSet(validation_images, validation_labels,\n",
    "                                 dtype=dtype)\n",
    "  data_sets.test = DataSet(test_images, test_labels, dtype=dtype)\n",
    "  return data_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data sets into the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data Set\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "The data set has been downloaded to ./MNIST_data, which has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image (28*28):\n",
    "\n",
    "\n",
    "![MNIST_Matrix](assets/MNIST-Matrix.png)\n",
    "\n",
    "\n",
    "We can flatten each array into a 28∗28=784 dimensional vector. Each component of the vector is a value between zero and one describing the intensity of the pixel. Thus, we generally think of MNIST as being a collection of 784-dimensional vectors.\n",
    "\n",
    "(The image will lose some information after been flattened, but it's not important for this example.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The image set\n",
    "\n",
    "Since each image is a 784 dimensional vector, the whole image set can be seen as a 2 dimensional tensor:\n",
    " - the first dimension is the index of images\n",
    " - the second dimension is the index of the pixels\n",
    " \n",
    "Then each element in the tensor reprecents one pixel of a image.\n",
    "\n",
    "![MNIST_mnist-train-xs](assets/mnist-train-xs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The lable set\n",
    "\n",
    "We use [0...9] array (the one-hot vector) to represent the numbers from 0 to 9, each image has one of 1 and nine of 0, then we got the lable set as the following:\n",
    "\n",
    "![MNIST_mnist-train-ys](assets/mnist-train-ys.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "\n",
    "Now we need to create the connection between the data set and the label set. By using softmax model, we can give different portabilities to different objects.\n",
    "\n",
    "More information about the softmax function can be found here: http://neuralnetworksanddeeplearning.com/chap3.html#softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "\n",
    "Create an `evidence` for the image by generating the weighted sum of each pixel value, and also adds a bias.\n",
    "\n",
    "Given image `x`, the `evidence` for number `i` is:\n",
    "\n",
    "\\begin{equation}\n",
    "evidence_x = \\sum_j w_{i,j}x_j + b_i\n",
    "\\end{equation}\n",
    "\n",
    " - w<sub>i</sub>: the weigh for number `i`\n",
    " - b<sub>i</sub>: the bias value for number `i`\n",
    " - x<sub>i</sub>: the pixel value of image `x`\n",
    " \n",
    "Then we can use softmax to transform this `evidence` into portabilities.\n",
    " \n",
    "\\begin{equation}\n",
    "y = softmax(evidence)\n",
    "\\end{equation}\n",
    "\n",
    "Actually we can discribe this as a data graph flow:\n",
    "\n",
    "![MNIST_softmax-regression-scalargraph](assets/softmax-regression-scalargraph.png)\n",
    "\n",
    "And also in the matrix form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{vmatrix}y_1\\\\y_2\\\\y_3\\\\...\\\\y_{10}\\end{vmatrix} = softmax \\left( \\begin{vmatrix}x_1\\\\x_2\\\\x_3\\\\...\\\\x_{784}\\end{vmatrix}\\begin{vmatrix}w_{1,1}&w_{1,2}&w_{1,3}&...&w_{1,10}\\\\w_{2,1}&w_{2,2}&w_{2,3}&...&w_{2,10}\\\\w_{3,1}&w_{3,2}&w_{3,3}&...&w_{3,10}\\\\...&...&...&...&...\\\\w_{784,1}&w_{784,2}&w_{784,3}&...&w_{784,10}\\end{vmatrix}\n",
    " +\n",
    "\\begin{vmatrix}b_1\\\\b_2\\\\b_3\\\\...\\\\b_{10}\\end{vmatrix}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Or in a simpler way:\n",
    "\n",
    "\\begin{equation}\n",
    "y = softmax \\left( Wx + b \\right)\n",
    "\\end{equation}\n",
    "\n",
    "So `softmax` is the `activation` function for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Regression with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define x as placeholders because we want to apply different images\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# Define W and b as variables, their values are what the model needs to learn\n",
    "W = tf.Variable(tf.random_normal([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Apply activation function\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "# And it's done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "To train the model, we need a `standard` to tell us that if the model is good or bad.\n",
    "\n",
    "In machine learning we typically define what it means for a model to be bad. We call this the `cost`, or the `loss`, and it represents how far off our model is from our desired outcome. We try to minimize that error, and the smaller the error margin, the better our model is.\n",
    "\n",
    "One very common, very nice function to determine the loss of a model is called `cross-entropy`. Cross-entropy arises from thinking about information compressing codes in information theory but it winds up being an important idea in lots of areas, from gambling to machine learning. It’s defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "H_{y’}y=\\sum_i y’_ilog(y)\n",
    "\\end{equation}\n",
    "\n",
    "Where `y` is our predicted probability distribution, and `y’` is the true distribution (the one-hot vector with the digit labels). In some rough sense, the cross-entropy is measuring how inefficient our predictions are for describing the truth. Going into more detail about cross-entropy is beyond the scope of this tutorial, but it’s well worth understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define y’\n",
    "y_ = tf.placeholder(\"float\", [None,10])\n",
    "# Define cross entropy\n",
    "cross_entropy = tf.reduce_sum(y_*tf.log(y))\n",
    "# Define train step\n",
    "# Here we use `gradient descent algorithm` for the regression\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# Init variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start session and train\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "print('done trainning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each step of the loop, we get a `batch` of 100 random data points from our training set. We run train_step feeding in the batches data to replace the placeholders.\n",
    "\n",
    "Using small batches of random data is called `stochastic training` – in this case, `stochastic gradient descent`. \n",
    "\n",
    "Ideally, we’d like to use all our data for every step of training because that would give us a better sense of what we should be doing, but that’s expensive. So, instead, we use a different subset every time. Doing this is cheap and has much of the same benefit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "Use the test data set to evaluate the model.\n",
    "\n",
    "Well, first let’s figure out where we predicted the correct label. `tf.argmax` is an extremely useful function which gives you the index of the highest entry in a tensor along some axis. For example, tf$argmax(y, 1L) is the label our model thinks is most likely for each input, while tf$argmax(y_, 1L) is the correct label. We can use tf$equal to check if our prediction matches the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the model\n",
    "\n",
    "The accurity is pretty bad, because The raw formulation of cross-entropy\n",
    "\n",
    "```\n",
    "cross_entropy = tf.reduce_sum(y_*tf.log(y))\n",
    "```\n",
    "can be numerically unstable.\n",
    "\n",
    "So here we use tf.losses.sparse_softmax_cross_entropy on the raw.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingweipeng/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-46bd2f86372a>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/jingweipeng/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/jingweipeng/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/jingweipeng/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/jingweipeng/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "0.9188\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('./MNIST')\n",
    "\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='x')\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.add(tf.matmul(x, W), b, name='model')\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "# The raw formulation of cross-entropy,\n",
    "#\n",
    "#   tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)),\n",
    "#                                 reduction_indices=[1]))\n",
    "#\n",
    "# can be numerically unstable.\n",
    "#\n",
    "# So here we use tf.losses.sparse_softmax_cross_entropy on the raw\n",
    "# outputs of 'y', and then average across the batch.\n",
    "cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=y)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Train\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), y_)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(\n",
    "  accuracy, feed_dict={\n",
    "      x: mnist.test.images,\n",
    "      y_: mnist.test.labels\n",
    "  }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is much better now but still very poor for production usage.\n",
    "\n",
    "To further improve the accuracy, we need to change the model.\n",
    "\n",
    "See who is the best in MNIST: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./models/mnist/1/saved_model.pb\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.util import compat\n",
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import signature_def_utils\n",
    "from tensorflow.saved_model import tag_constants\n",
    "\n",
    "builder = saved_model_builder.SavedModelBuilder('./models/mnist/1')\n",
    "builder.add_meta_graph_and_variables(\n",
    "      sess, [tag_constants.SERVING],\n",
    "      signature_def_map={\n",
    "        \"model\": tf.saved_model.signature_def_utils.predict_signature_def(\n",
    "          inputs= {\"x\": x },\n",
    "          outputs= {\"y\": y }\n",
    "        )\n",
    "      })\n",
    "builder.add_meta_graph([tag_constants.SERVING], strip_default_attrs=True)\n",
    "builder.save()\n",
    "print('Model saved')\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conver a image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "\n",
    "\n",
    "def imageprepare(argv):\n",
    "    \"\"\"\n",
    "    This function returns the pixel values.\n",
    "    The imput is a png file location.\n",
    "    \"\"\"\n",
    "    im = Image.open(argv).convert('L')\n",
    "    width = float(im.size[0])\n",
    "    height = float(im.size[1])\n",
    "    newImage = Image.new('L', (28, 28), (255))  # creates white canvas of 28x28 pixels\n",
    "\n",
    "    if width > height:  # check which dimension is bigger\n",
    "        # Width is bigger. Width becomes 20 pixels.\n",
    "        nheight = int(round((20.0 / width * height), 0))  # resize height according to ratio width\n",
    "        if (nheight == 0):  # rare case but minimum is 1 pixel\n",
    "            nheight = 1\n",
    "            # resize and sharpen\n",
    "        img = im.resize((20, nheight), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        wtop = int(round(((28 - nheight) / 2), 0))  # calculate horizontal position\n",
    "        newImage.paste(img, (4, wtop))  # paste resized image on white canvas\n",
    "    else:\n",
    "        # Height is bigger. Heigth becomes 20 pixels.\n",
    "        nwidth = int(round((20.0 / height * width), 0))  # resize width according to ratio height\n",
    "        if (nwidth == 0):  # rare case but minimum is 1 pixel\n",
    "            nwidth = 1\n",
    "            # resize and sharpen\n",
    "        img = im.resize((nwidth, 20), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        wleft = int(round(((28 - nwidth) / 2), 0))  # caculate vertical pozition\n",
    "        newImage.paste(img, (wleft, 4))  # paste resized image on white canvas\n",
    "\n",
    "    # newImage.save(\"sample.png\n",
    "\n",
    "    tv = list(newImage.getdata())  # get pixel values\n",
    "\n",
    "    # normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.\n",
    "    tva = [(255 - x) * 1.0 / 255.0 for x in tv]\n",
    "#     print(tva)\n",
    "    return tva\n",
    "\n",
    "x1=imageprepare('./assets/5.jpg') # 28*28\n",
    "x2=imageprepare('./assets/4.png') # screenshot\n",
    "x3=imageprepare('./assets/2.png') #file path here\n",
    "print(len(x3))# mnist IMAGES are 28x28=784 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a saved model and test the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/mnist/1/variables/variables\n",
      "[5 4 2]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.saved_model import tag_constants\n",
    "\n",
    "export_dir = './models/mnist/1'\n",
    "with tf.Session(graph=tf.Graph()) as sess:  \n",
    "  tf.saved_model.loader.load(sess, [tag_constants.SERVING], export_dir)\n",
    "  graph = tf.get_default_graph()\n",
    "  x = graph.get_tensor_by_name('x:0')\n",
    "  y = graph.get_tensor_by_name('model:0')\n",
    "  result = tf.argmax(tf.nn.softmax(y), 1)\n",
    "  print(sess.run(\n",
    "  result, feed_dict={\n",
    "      x: [x1, x2, x3],\n",
    "  }))\n",
    "  print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
