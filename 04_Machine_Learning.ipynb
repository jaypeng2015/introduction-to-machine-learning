{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This course cs229 offered by Stanford can help you understand Machine Learning better:\n",
    "\n",
    "\n",
    "https://www.youtube.com/playlist?list=PL7FnO5AWye3YU4LeayDKP1AhOqJBAdBYt\n",
    "\n",
    "The latest course website is at: http://cs229.stanford.edu/\n",
    "\n",
    "It's been 10 years late to this course so you will probably find that some of the terms have change a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keynotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 1\n",
    "\n",
    "### The definition of Machine Learning:\n",
    "\n",
    "Arthul Samuel (1959): Fields of study that gives computers the ability to learn without being explicitly programmed.\n",
    "\n",
    "\n",
    "Tom M. Mitchell (1998): A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.\n",
    "\n",
    "### Main focus of this course\n",
    " - Supervised Learning\n",
    "   - regression problems\n",
    "   - classification problems\n",
    " - Learning Theory\n",
    " - Unsupervised Learning\n",
    "   - clustering problem\n",
    "   - social network analysis\n",
    "   - market segmentation\n",
    "   - astronomical data analysis\n",
    " - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2\n",
    " - Linear Regression\n",
    " - Gradient Descent\n",
    " - Normal Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "This is commanly used in `Supervised Learning`.\n",
    "\n",
    "Take house price prediction for example. \n",
    "\n",
    "Let's say if we have different variables as features of the house, such as\n",
    " - x<sub>1</sub>: land size\n",
    " - x<sub>2</sub>: number of bedrooms\n",
    " \n",
    "Then a prediction function, which is also called `hypothesis`, can be discribed as:\n",
    "    \n",
    "\\begin{equation*}\n",
    "h\\left( x\\right) = h_{\\theta}(x) = \\sum_{0}^n \\theta_i x_i = \\theta^Tx\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "- define x<sub>0</sub>=1\n",
    "- n is the number of variables which is 2 here\n",
    "- w is the weight of the variables\n",
    "- ùúÉ's are called parameters\n",
    "\n",
    "To train this `hypothesis`, we define \n",
    "\n",
    "\\begin{equation*}\n",
    "J(\\theta)=\\frac12\\sum_i^m(h_\\theta(x_i)-y_i)^2\n",
    "\\end{equation*}\n",
    "\n",
    "- y: the actual price of the house\n",
    "- m: the number of traning data\n",
    "\n",
    "J(ùúÉ) is called the `cost`, then the next step will be using some algorism to minimise the cost.\n",
    " \n",
    "- start with some ùúÉ, for example ùúÉ=0\n",
    "- keep changing ùúÉ to reduce J(ùúÉ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "#### Batch Gradient Descent\n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta_i:=\\theta_i-\\alpha{\\frac{\\partial}{\\partial\\theta_i}}J(\\theta)\n",
    "\\end{equation*}\n",
    "\n",
    "- ùõº: is called the learning rate which controls how large the step takes\n",
    "\n",
    "If we have only one train example, then\n",
    "\n",
    "\\begin{equation*}\n",
    "{\\frac{\\partial}{\\partial\\theta_i}}J(\\theta) = {\\frac{\\partial}{\\partial\\theta_i}}\\frac12\\sum_i^1(h_\\theta(x_i)-y_i)^2\n",
    "= {\\frac{\\partial}{\\partial\\theta_i}}\\frac12(h_\\theta(x)-y)^2\n",
    "= 2 \\times \\frac12 \\left( h_\\theta\\left( x_i \\right)-y_i \\right) \\times \\frac{\\partial}{\\partial\\theta_i}\n",
    "\\left( h_\\theta\\left( x_i \\right)-y_i \\right)\n",
    "= \\left( h_\\theta\\left( x_i \\right)-y_i \\right)x_i\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Generally for m data examples, the whole algorism is described as:\n",
    "\n",
    "Repeat until convergence:\n",
    "    \n",
    "\\begin{equation*}\n",
    "\\theta_i := \\theta_i - \\alpha\\sum_{j=1}^m(h_\\theta(x_{ij})-y_{ij})x_{ij}\n",
    "\\end{equation*}\n",
    "\n",
    "This algorism is called `Batch Gradient Descent`, and for each step it needs to consume the whole trainning set.\n",
    "\n",
    "When you have a large trainning set, for example, 1 million, then you may need to consider another algorism which is called `Stochastic Gradient Descent`, or `Incremental Gradient Descent`\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "\n",
    "Repeat until convergence:\n",
    "\n",
    "    for j from i to m:\n",
    "\\begin{equation*}\n",
    "\\theta_i := \\theta_i - \\alpha(h_\\theta(x_{ij})-y_{ij})x_{ij}\n",
    "\\end{equation*}\n",
    "\n",
    "This runs much faster on large trainning sets, and the results tend to the region near the globla minimum, not exactly the globla minimum but that's good enough.\n",
    "\n",
    "### Normal Equations\n",
    "\n",
    "A close form but simplified solution to Batch Gradient Descent when the expeted output is in R<sup>1</sup>.\n",
    "\n",
    "Define:\n",
    "\\begin{equation*}\n",
    "\\nabla J(\\theta) = \\begin{vmatrix}\\frac{\\partial}{\\partial\\theta_0}\\\\\\frac{\\partial}{\\partial\\theta_1}\\\\...\\\\\\frac{\\partial}{\\partial\\theta_n}\\end{vmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Then\n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta := \\theta - \\alpha\\nabla J(\\theta)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "More generally:\n",
    "\n",
    "If you have a function f to map the space of m by n matrix A to a vector, \n",
    "\n",
    "Define:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\nabla_A f(A) = \\begin{vmatrix}\n",
    "\\frac{\\partial f}{\\partial A_{11}}&\\frac{\\partial f}{\\partial A_{12}}&...&\\frac{\\partial f}{\\partial A_{1n}}\\\\\n",
    "\\frac{\\partial f}{\\partial A_{21}}&\\frac{\\partial f}{\\partial A_{22}}&...&\\frac{\\partial f}{\\partial A_{2n}}\\\\...&...&...&...\\\\\n",
    "\\frac{\\partial f}{\\partial A_{m1}}&\\frac{\\partial f}{\\partial A_{m2}}&...&\\frac{\\partial f}{\\partial A_{mn}}\\end{vmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Also if m = n, define `trace`:\n",
    "\n",
    "\\begin{equation*}Tr(A) = \\sum_{i=1}^nA_{ii}\\end{equation*}\n",
    "\n",
    "Then the facts:\n",
    "\\begin{equation*}Tr(a) = a \\space \\space // \\space n=1\\end{equation*}\n",
    "\\begin{equation*}Tr(A) = Tr(A^T)\\end{equation*}\n",
    "\\begin{equation*}Tr(AB) = Tr(BA)\\end{equation*}\n",
    "\\begin{equation*}Tr(ABC) = Tr(CAB) = Tr(BCA)\\end{equation*}\n",
    "\n",
    "Also Suppose f(A) = Tr(AB), then:\n",
    "\n",
    "\\begin{equation*}\\nabla_A f(A) = \\nabla_A Tr(AB) = B^T\\end{equation*}\n",
    "\n",
    "And finally a tricky one:\n",
    "\\begin{equation*}\\nabla_A Tr(ABA^TC) = CBA + C^TAB^T\\end{equation*}\n",
    "\n",
    "By having all these facts above, define matrix X to be all the inputs from traninig set (the `design matrix`):\n",
    "\n",
    "\\begin{equation*}X = \\begin{vmatrix}x_1^T\\\\x_2^T\\\\..\\\\x_m^T\\end{vmatrix}\\end{equation*}\n",
    "\n",
    "Then:\n",
    "\n",
    "\n",
    "\\begin{equation*}X\\theta = \\begin{vmatrix}x_1^T\\\\x_2^T\\\\..\\\\x_m^T\\end{vmatrix}\\theta\n",
    "= \\begin{vmatrix}x_1^T\\theta\\\\x_2^T\\theta\\\\...\\\\x_m^T\\theta\\end{vmatrix}\n",
    "= \\begin{vmatrix}h_\\theta(x_1)\\\\h_\\theta(x_2)\\\\...\\\\h_\\theta(x_m)\\end{vmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Define vector y as all the target values:\n",
    "\n",
    "\\begin{equation*}y = \\begin{vmatrix}y_1\\\\y_2\\\\..\\\\y_m\\end{vmatrix}\\end{equation*}\n",
    "\n",
    "Then:\n",
    "\n",
    "\\begin{equation*}X\\theta - y = \\begin{vmatrix}h_\\theta(x_1)-y_1\\\\h_\\theta(x_2)-y_2\\\\...\\\\h_\\theta(x_m)-y_m\\end{vmatrix}\\end{equation*}\n",
    "\n",
    "Recall:\n",
    "\n",
    "\\begin{equation*} z^Tz = \\sum_iz_i^2\\end{equation*}\n",
    "\n",
    "Then:\n",
    "\n",
    "\\begin{equation*}\\frac12(X\\theta - y)^T(X\\theta - y) = \\frac12\\sum_{i=1}^m (h_\\theta(x_i)-y_i)^2 = J(\\theta)\\end{equation*}\n",
    "\n",
    "Try to solve **‚àá<sub>ùúÉ</sub>ùêΩ(ùúÉ) = 0** by using the facts above:\n",
    "\n",
    "\n",
    "\n",
    "\\begin{gather*}\\nabla_\\theta J(\\theta) \n",
    "= \\nabla_\\theta \\frac12(X\\theta - y)^T(X\\theta - y) \\\\\n",
    "= \\frac12 \\nabla_\\theta (X\\theta - y)^T(X\\theta - y) \\\\\n",
    "= \\frac12 \\nabla_\\theta (\\theta^TX^TX\\theta - \\theta^TX^Ty - y^TX\\theta + y^Ty) \\\\\n",
    "= \\frac12 \\nabla_\\theta Tr(\\theta^TX^TX\\theta - \\theta^TX^Ty - y^TX\\theta + y^Ty)  \\\\\n",
    "= \\frac12 ((\\nabla_\\theta Tr(\\theta\\theta^TX^TX) - \\nabla_\\theta Tr(y^TX\\theta) - \\nabla_\\theta Tr(y^TX\\theta) + 0)  \\\\\n",
    "= \\frac12 (X^TX\\theta + X^TX\\theta  - X^Ty - X^Ty) \\\\\n",
    "= X^TX\\theta - X^Ty) = 0 \n",
    "\\end{gather*}\n",
    "\n",
    "So \n",
    "\n",
    "\\begin{equation*} X^TX\\theta = X^Ty\\end{equation*}\n",
    "\n",
    "The equation is called the `normal equations` and we can slove:\n",
    "\n",
    "\\begin{equation*} \\theta = (X^TX)^{-1}X^Ty\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
